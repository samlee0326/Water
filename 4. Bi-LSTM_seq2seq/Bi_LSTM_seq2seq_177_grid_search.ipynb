{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impressed-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hydroeval as he\n",
    "from sklearn.preprocessing import *\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score,max_error\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "import warnings\n",
    "from keras.optimizers import *\n",
    "import copy\n",
    "import os\n",
    "from keras.callbacks import *\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collaborative-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"font.family\"] = 'Nanum Brush Script OTF'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "CB91_Blue = '#2CBDFE'\n",
    "CB91_Green = '#47DBCD'\n",
    "CB91_Pink = '#F3A0F2'\n",
    "CB91_Purple = '#9D2EC5'\n",
    "CB91_Violet = '#661D98'\n",
    "CB91_Amber = '#F5B14C'\n",
    "\n",
    "color_list = [CB91_Blue, \n",
    "              CB91_Pink, \n",
    "              CB91_Green, \n",
    "              CB91_Amber,\n",
    "              CB91_Purple, \n",
    "              CB91_Violet]\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metropolitan-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather_dam_wrn.csv',encoding='cp949',index_col=[0])\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "satellite-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_lagger(df,x,num_lags):\n",
    "    df2 = copy.deepcopy(df)\n",
    "    for lag in range(1,num_lags+1):\n",
    "        df2[x+'_'+str(lag)] = df2[x].shift(lag)\n",
    "        \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sized-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anticipated-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['대설경보', '태풍경보', '폭염경보', '한파경보'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "appointed-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['강우량(mm)+1']] = df[['강우량(mm)']].shift(-1)\n",
    "#df[['강우량(mm)+2']] = df[['강우량(mm)']].shift(-2)\n",
    "df = col_lagger(df,'강우량(mm)',7)\n",
    "df = col_lagger(df,'유입량(㎥/s)',7)\n",
    "df = col_lagger(df,'최저기온(°C)',7)\n",
    "df = col_lagger(df,'최고기온(°C)',7)\n",
    "df = col_lagger(df,'평균 풍속(m/s)',7)\n",
    "df = col_lagger(df,'호우경보',7)\n",
    "df = col_lagger(df,'평균 상대습도(%)',7)\n",
    "df = col_lagger(df,'합계 일사량(MJ/m2)',7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "emerging-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['최저기온(°C)', '최고기온(°C)', \n",
    "                      '평균 풍속(m/s)', '평균 상대습도(%)',\n",
    "                      '합계 일사량(MJ/m2)','호우경보'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "leading-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[c for c in df.columns if '유입량(㎥/s)'  != c]].values\n",
    "y = df[['유입량(㎥/s)']].values\n",
    "\n",
    "data_stacked = np.hstack((X,y))\n",
    "\n",
    "del X\n",
    "del y\n",
    "X,y = split_sequences(data_stacked,1,2)\n",
    "train_length = int(df.shape[0]*0.8)\n",
    "\n",
    "train_X , train_y = X[:train_length, :] , y[:train_length, :]\n",
    "test_X , test_y = X[train_length:, :] , y[train_length:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-dutch",
   "metadata": {},
   "source": [
    "# Scaling X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "secret-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1x = StandardScaler()\n",
    "scaler1y = StandardScaler()\n",
    "\n",
    "#Scale X\n",
    "train_X_prescaled = train_X.reshape(train_X.shape[0],-1)\n",
    "train_X1 = scaler1x.fit_transform(train_X_prescaled).reshape(train_X_prescaled.shape[0],1,train_X_prescaled.shape[-1])\n",
    "\n",
    "test_X_prescaled = test_X.reshape(test_X.shape[0],-1)\n",
    "test_X1 = scaler1x.transform(test_X_prescaled).reshape(test_X.shape[0],1,test_X.shape[-1])\n",
    "\n",
    "#Scale y\n",
    "train_y_prescaled = train_y.reshape(train_y.shape[0],-1)\n",
    "train_y1 = scaler1y.fit_transform(train_y_prescaled).reshape(train_y.shape[0],1,train_y.shape[-1])\n",
    "\n",
    "test_y_prescaled = test_y.reshape(test_y.shape[0],-1)\n",
    "test_y1 = scaler1y.transform(test_y_prescaled).reshape(test_y.shape[0],1,test_y.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-psychiatry",
   "metadata": {},
   "source": [
    "# Testing with Bi_LSTM seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "renewable-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "passing-duration",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d740dc27f0c41f2a8ff48cfdc4c14f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch Size:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Learning Rate:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size 38, Batch Size: 64, Learning Rate:0.1\n",
      "\n",
      "\n",
      "RMSE 1일차: 1065271000.78\n",
      "MAE 1일차: 52053484.31\n",
      "R2 1일: -47241469713544.14\n",
      "Max Error 1일: 26986082502.7\n",
      "\n",
      "\n",
      "RMSE 2일차: 1085431420.43\n",
      "MAE 2일차: 56593813.54\n",
      "R2 2일: -49045079890418.52\n",
      "Max Error 2일: 24240076878.11\n",
      "\n",
      "\n",
      "NSE: [0.50826524 0.48947681]\n",
      "Saved model to disk\n",
      "\n",
      "Size 38, Batch Size: 64, Learning Rate:0.01\n",
      "\n",
      "\n",
      "RMSE 1일차: 262.50\n",
      "MAE 1일차: 95.89\n",
      "R2 1일: -1.8685027187502952\n",
      "Max Error 1일: 2777.514599609375\n",
      "\n",
      "\n",
      "RMSE 2일차: 311.85\n",
      "MAE 2일차: 100.02\n",
      "R2 2일: -3.04843857527578\n",
      "Max Error 2일: 4985.08107421875\n",
      "\n",
      "\n",
      "NSE: [0.64968691 0.50557413]\n",
      "Saved model to disk\n",
      "\n",
      "Size 38, Batch Size: 64, Learning Rate:0.001\n",
      "\n",
      "\n",
      "RMSE 1일차: 49.01\n",
      "MAE 1일차: 15.56\n",
      "R2 1일: 0.9000163400955414\n",
      "Max Error 1일: 699.0261279296874\n",
      "\n",
      "\n",
      "RMSE 2일차: 70.08\n",
      "MAE 2일차: 18.34\n",
      "R2 2일: 0.7955455424697939\n",
      "Max Error 2일: 1375.18703125\n",
      "\n",
      "\n",
      "NSE: [0.95690011 0.9118634 ]\n",
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Learning Rate:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size 38, Batch Size: 128, Learning Rate:0.1\n",
      "\n",
      "\n",
      "RMSE 1일차: 4196195.12\n",
      "MAE 1일차: 803332.40\n",
      "R2 1일: -733017832.5092229\n",
      "Max Error 1일: 91735920.47\n",
      "\n",
      "\n",
      "RMSE 2일차: 4133908.77\n",
      "MAE 2일차: 660674.85\n",
      "R2 2일: -711397677.744174\n",
      "Max Error 2일: 133463103.8\n",
      "\n",
      "\n",
      "NSE: [0.49243088 0.50738728]\n",
      "Saved model to disk\n",
      "\n",
      "Size 38, Batch Size: 128, Learning Rate:0.01\n",
      "\n",
      "\n",
      "RMSE 1일차: 57.52\n",
      "MAE 1일차: 17.14\n",
      "R2 1일: 0.8622693624906312\n",
      "Max Error 1일: 864.6687890625\n",
      "\n",
      "\n",
      "RMSE 2일차: 71.46\n",
      "MAE 2일차: 19.56\n",
      "R2 2일: 0.7874509942772348\n",
      "Max Error 2일: 1250.59572265625\n",
      "\n",
      "\n",
      "NSE: [0.93862112 0.905276  ]\n",
      "Saved model to disk\n",
      "\n",
      "Size 38, Batch Size: 128, Learning Rate:0.001\n",
      "\n",
      "\n",
      "RMSE 1일차: 50.25\n",
      "MAE 1일차: 15.84\n",
      "R2 1일: 0.8949016830860324\n",
      "Max Error 1일: 806.8538867187499\n",
      "\n",
      "\n",
      "RMSE 2일차: 63.90\n",
      "MAE 2일차: 18.13\n",
      "R2 2일: 0.8300150327043022\n",
      "Max Error 2일: 1047.408466796875\n",
      "\n",
      "\n",
      "NSE: [0.95245891 0.9231053 ]\n",
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Learning Rate:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size 38, Batch Size: 256, Learning Rate:0.1\n",
      "\n",
      "\n",
      "RMSE 1일차: 4653.72\n",
      "MAE 1일차: 585.13\n",
      "R2 1일: -900.5796645743117\n",
      "Max Error 1일: 94256.59125\n",
      "\n",
      "\n",
      "RMSE 2일차: 7462.92\n",
      "MAE 2일차: 803.79\n",
      "R2 2일: -2317.50549909761\n",
      "Max Error 2일: 189205.93875\n",
      "\n",
      "\n",
      "NSE: [0.71982445 0.27947877]\n",
      "Saved model to disk\n",
      "\n",
      "Size 38, Batch Size: 256, Learning Rate:0.01\n",
      "\n",
      "\n",
      "RMSE 1일차: 66.46\n",
      "MAE 1일차: 17.55\n",
      "R2 1일: 0.8161492718243909\n",
      "Max Error 1일: 1048.4978320312498\n",
      "\n",
      "\n",
      "RMSE 2일차: 75.35\n",
      "MAE 2일차: 19.20\n",
      "R2 2일: 0.7636318214063293\n",
      "Max Error 2일: 1314.359638671875\n",
      "\n",
      "\n",
      "NSE: [0.92199367 0.89970806]\n",
      "Saved model to disk\n",
      "\n",
      "Size 38, Batch Size: 256, Learning Rate:0.001\n",
      "\n",
      "\n",
      "RMSE 1일차: 44.17\n",
      "MAE 1일차: 14.94\n",
      "R2 1일: 0.9187830097333952\n",
      "Max Error 1일: 574.7243701171874\n",
      "\n",
      "\n",
      "RMSE 2일차: 58.59\n",
      "MAE 2일차: 17.11\n",
      "R2 2일: 0.8570843232582444\n",
      "Max Error 2일: 1090.51076171875\n",
      "\n",
      "\n",
      "NSE: [0.96340189 0.93559729]\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "\n",
    "lr=[0.1,0.01,0.001]\n",
    "\n",
    "\n",
    "for batch in tqdm_notebook([64,128,256], desc='Batch Size'):\n",
    "    for rate in tqdm_notebook(lr,leave=False, desc='Learning Rate'):\n",
    "        #print(f'batch: {batch}')\n",
    "        #print(f'rate: {rate}')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  min_delta=0.1,\n",
    "                                  patience=5, min_lr=1e-5, vebose=1)\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = rate)\n",
    "        encoder_inputs = Input(shape=(train_X1.shape[1], train_X1.shape[2]))\n",
    "\n",
    "\n",
    "        encoder_l1 = Bidirectional(LSTM(177,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        return_state=True,\n",
    "                                        recurrent_dropout=0.5))\n",
    "\n",
    "\n",
    "\n",
    "        encoder_outputs1 = encoder_l1(encoder_inputs)\n",
    "        encoder_states1  = encoder_outputs1[1:]\n",
    "\n",
    "        encoder_l2 = Bidirectional(LSTM(177,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',\n",
    "                                        return_state=True,\n",
    "                                        recurrent_dropout=0.5))\n",
    "\n",
    "\n",
    "\n",
    "        encoder_outputs2 = encoder_l2(encoder_outputs1[0])\n",
    "        encoder_states2 = encoder_outputs2[1:]\n",
    "\n",
    "        encoder_l3 = Bidirectional(LSTM(177,\n",
    "                                        return_sequences=False,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        return_state=True,\n",
    "                                        recurrent_dropout=0.5))\n",
    "\n",
    "\n",
    "        encoder_outputs3 = encoder_l3(encoder_outputs2[0])\n",
    "        encoder_states3 = encoder_outputs3[1:]\n",
    "\n",
    "        #Decoder\n",
    "        decoder_inputs = RepeatVector(1)(encoder_outputs3[0])\n",
    "\n",
    "        decoder_l1 = Bidirectional(LSTM(177,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        recurrent_dropout=0.5))(decoder_inputs,initial_state=encoder_states1)\n",
    "\n",
    "\n",
    "        decoder_l2 = Bidirectional(LSTM(177,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        recurrent_dropout=0.5))(decoder_l1,initial_state=encoder_states2)\n",
    "\n",
    "\n",
    "        decoder_l3 = Bidirectional(LSTM(177,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        recurrent_dropout=0.5))(decoder_l2,initial_state=encoder_states3)\n",
    "\n",
    "\n",
    "        decoder_outputs2 = TimeDistributed(Dense(2))(decoder_l3)\n",
    "\n",
    "        model = tf.keras.models.Model(encoder_inputs,decoder_outputs2)\n",
    "        model.compile(loss='mse',optimizer = opt,metrics = ['mse'])\n",
    "        \n",
    "        history = model.fit(train_X1, train_y1, \n",
    "                            epochs=3000, \n",
    "                            verbose=0,\n",
    "                            batch_size=batch,\n",
    "                            shuffle=False,\n",
    "                            validation_split=0.1,\n",
    "                            callbacks=[reduce_lr])\n",
    "\n",
    "        print()\n",
    "        print(f'Size 38, Batch Size: {batch}, Learning Rate:{rate}\\n\\n') \n",
    "        \n",
    "        y_hat = model.predict(test_X1)\n",
    "\n",
    "        y_hat_inv = scaler1y.inverse_transform(y_hat).reshape(y_hat.shape[0],-1)\n",
    "        test_y1_inv = scaler1y.inverse_transform(test_y1).reshape(test_y1.shape[0],-1)\n",
    "\n",
    "        print(f'RMSE 1일차: {mean_squared_error(test_y1_inv[:,0],y_hat_inv[:,0],squared=False):.2f}')\n",
    "        print(f'MAE 1일차: {mean_absolute_error(test_y1_inv[:,0],y_hat_inv[:,0]):.2f}')\n",
    "        print(f'R2 1일: {r2_score(test_y1_inv[:,0],y_hat_inv[:,0])}')\n",
    "        print(f'Max Error 1일: {max_error(test_y1_inv[:,0],y_hat_inv[:,0])}')\n",
    "        print()\n",
    "        print()\n",
    "        print(f'RMSE 2일차: {mean_squared_error(test_y1_inv[:,1],y_hat_inv[:,1],squared=False):.2f}')\n",
    "        print(f'MAE 2일차: {mean_absolute_error(test_y1_inv[:,1],y_hat_inv[:,1]):.2f}')\n",
    "        print(f'R2 2일: {r2_score(test_y1_inv[:,1],y_hat_inv[:,1])}')\n",
    "        print(f'Max Error 2일: {max_error(test_y1_inv[:,1],y_hat_inv[:,1])}')\n",
    "        print()\n",
    "        print()\n",
    "        print(f'NSE: {he.nse(test_y1_inv,y_hat_inv)}')\n",
    "        model_yaml = model.to_yaml()\n",
    "        with open(f\"bi_lstm177_lr{rate}_batch_{batch}.yaml\", \"w\") as yaml_file:\n",
    "            yaml_file.write(model_yaml)\n",
    "\n",
    "        model.save_weights(f\"bi_lstm177_lr{rate}_batch_{batch}.h5\")\n",
    "        print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
