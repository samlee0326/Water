{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impressed-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.CommonFunctions import col_lagger,split_sequences\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hydroeval as he\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "import warnings\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collaborative-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"font.family\"] = 'Nanum Brush Script OTF'\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "CB91_Blue = '#2CBDFE'\n",
    "CB91_Green = '#47DBCD'\n",
    "CB91_Pink = '#F3A0F2'\n",
    "CB91_Purple = '#9D2EC5'\n",
    "CB91_Violet = '#661D98'\n",
    "CB91_Amber = '#F5B14C'\n",
    "\n",
    "color_list = [CB91_Blue, \n",
    "              CB91_Pink, \n",
    "              CB91_Green, \n",
    "              CB91_Amber,\n",
    "              CB91_Purple, \n",
    "              CB91_Violet]\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "metropolitan-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..//Processed_data//weather_dam_wrn.csv',encoding='cp949',index_col=[0])\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "appointed-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['강우량(mm)+1']] = df[['강우량(mm)']].shift(-1)\n",
    "#df[['강우량(mm)+2']] = df[['강우량(mm)']].shift(-2)\n",
    "df = col_lagger(df,'강우량(mm)',7)\n",
    "df = col_lagger(df,'유입량(㎥/s)',7)\n",
    "df = col_lagger(df,'최저기온(°C)',7)\n",
    "df = col_lagger(df,'최고기온(°C)',7)\n",
    "df = col_lagger(df,'평균 풍속(m/s)',7)\n",
    "df = col_lagger(df,'호우경보',7)\n",
    "df = col_lagger(df,'평균 상대습도(%)',7)\n",
    "df = col_lagger(df,'합계 일사량(MJ/m2)',7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "emerging-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['최저기온(°C)', '최고기온(°C)', \n",
    "                      '평균 풍속(m/s)', '평균 상대습도(%)',\n",
    "                      '합계 일사량(MJ/m2)','호우경보'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "viral-survivor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ruled-cheat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['강우량(mm)', '유입량(㎥/s)', '강우량(mm)+1', '강우량(mm)_1', '강우량(mm)_2',\n",
       "       '강우량(mm)_3', '강우량(mm)_4', '강우량(mm)_5', '강우량(mm)_6', '강우량(mm)_7',\n",
       "       '유입량(㎥/s)_1', '유입량(㎥/s)_2', '유입량(㎥/s)_3', '유입량(㎥/s)_4', '유입량(㎥/s)_5',\n",
       "       '유입량(㎥/s)_6', '유입량(㎥/s)_7', '최저기온(°C)_1', '최저기온(°C)_2', '최저기온(°C)_3',\n",
       "       '최저기온(°C)_4', '최저기온(°C)_5', '최저기온(°C)_6', '최저기온(°C)_7', '최고기온(°C)_1',\n",
       "       '최고기온(°C)_2', '최고기온(°C)_3', '최고기온(°C)_4', '최고기온(°C)_5', '최고기온(°C)_6',\n",
       "       '최고기온(°C)_7', '평균 풍속(m/s)_1', '평균 풍속(m/s)_2', '평균 풍속(m/s)_3',\n",
       "       '평균 풍속(m/s)_4', '평균 풍속(m/s)_5', '평균 풍속(m/s)_6', '평균 풍속(m/s)_7',\n",
       "       '호우경보_1', '호우경보_2', '호우경보_3', '호우경보_4', '호우경보_5', '호우경보_6', '호우경보_7',\n",
       "       '평균 상대습도(%)_1', '평균 상대습도(%)_2', '평균 상대습도(%)_3', '평균 상대습도(%)_4',\n",
       "       '평균 상대습도(%)_5', '평균 상대습도(%)_6', '평균 상대습도(%)_7', '합계 일사량(MJ/m2)_1',\n",
       "       '합계 일사량(MJ/m2)_2', '합계 일사량(MJ/m2)_3', '합계 일사량(MJ/m2)_4',\n",
       "       '합계 일사량(MJ/m2)_5', '합계 일사량(MJ/m2)_6', '합계 일사량(MJ/m2)_7'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "leading-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[c for c in df.columns if '유입량(㎥/s)'  != c]].values\n",
    "y = df[['유입량(㎥/s)']].values\n",
    "\n",
    "data_stacked = np.hstack((X,y))\n",
    "\n",
    "del X\n",
    "del y\n",
    "X,y = split_sequences(data_stacked,1,2)\n",
    "train_length = int(df.shape[0]*0.8)\n",
    "\n",
    "train_X , train_y = X[:train_length, :] , y[:train_length, :]\n",
    "test_X , test_y = X[train_length:, :] , y[train_length:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-dutch",
   "metadata": {},
   "source": [
    "# Scaling X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "secret-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1x = StandardScaler()\n",
    "scaler1y = StandardScaler()\n",
    "\n",
    "#Scale X\n",
    "train_X_prescaled = train_X.reshape(train_X.shape[0],-1)\n",
    "train_X1 = scaler1x.fit_transform(train_X_prescaled).reshape(train_X_prescaled.shape[0],1,train_X_prescaled.shape[-1])\n",
    "\n",
    "test_X_prescaled = test_X.reshape(test_X.shape[0],-1)\n",
    "test_X1 = scaler1x.transform(test_X_prescaled).reshape(test_X.shape[0],1,test_X.shape[-1])\n",
    "\n",
    "#Scale y\n",
    "train_y_prescaled = train_y.reshape(train_y.shape[0],-1)\n",
    "train_y1 = scaler1y.fit_transform(train_y_prescaled).reshape(train_y.shape[0],1,train_y.shape[-1])\n",
    "\n",
    "test_y_prescaled = test_y.reshape(test_y.shape[0],-1)\n",
    "test_y1 = scaler1y.transform(test_y_prescaled).reshape(test_y.shape[0],1,test_y.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "approximate-ontario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4520, 1, 58)\n",
      "(1130, 1, 58)\n",
      "(4520, 1, 2)\n",
      "(1130, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_X1.shape)\n",
    "print(test_X1.shape)\n",
    "print(train_y1.shape)\n",
    "print(test_y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-psychiatry",
   "metadata": {},
   "source": [
    "# Testing with Bi_LSTM seq2seq(38,38,38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=[0.1,0.01,0.001]\n",
    "batches = [64,128,256]\n",
    "\n",
    "for batch in tqdm_notebook(batches, desc='Batch Size'):\n",
    "    for rate in tqdm_notebook(lr,leave=False, desc='Learning Rate'):\n",
    "        print('*'*80)\n",
    "        print(f'\\nBatch Size: {batch}...Learning Rate: {rate}\\n')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  min_delta=0.1,\n",
    "                                  patience=5, min_lr=1e-5, vebose=1)\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = rate)\n",
    "        encoder_inputs = Input(shape=(train_X1.shape[1], train_X1.shape[2]))\n",
    "\n",
    "\n",
    "        encoder_l1 = Bidirectional(LSTM(118,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        return_state=True,\n",
    "                                        recurrent_dropout=0.5))\n",
    "\n",
    "\n",
    "\n",
    "        encoder_outputs1 = encoder_l1(encoder_inputs)\n",
    "        encoder_states1  = encoder_outputs1[1:]\n",
    "\n",
    "        encoder_l2 = Bidirectional(LSTM(118,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',\n",
    "                                        return_state=True,\n",
    "                                        recurrent_dropout=0.5))\n",
    "\n",
    "\n",
    "\n",
    "        encoder_outputs2 = encoder_l2(encoder_outputs1[0])\n",
    "        encoder_states2 = encoder_outputs2[1:]\n",
    "\n",
    "        encoder_l3 = Bidirectional(LSTM(118,\n",
    "                                        return_sequences=False,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        return_state=True,\n",
    "                                        recurrent_dropout=0.5))\n",
    "\n",
    "\n",
    "        encoder_outputs3 = encoder_l3(encoder_outputs2[0])\n",
    "        encoder_states3 = encoder_outputs3[1:]\n",
    "\n",
    "        #Decoder\n",
    "        decoder_inputs = RepeatVector(1)(encoder_outputs3[0])\n",
    "\n",
    "        decoder_l1 = Bidirectional(LSTM(118,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        recurrent_dropout=0.5))(decoder_inputs,initial_state=encoder_states1)\n",
    "\n",
    "\n",
    "        decoder_l2 = Bidirectional(LSTM(118,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        recurrent_dropout=0.5))(decoder_l1,initial_state=encoder_states2)\n",
    "\n",
    "\n",
    "        decoder_l3 = Bidirectional(LSTM(118,\n",
    "                                        return_sequences=True,\n",
    "                                        activation='selu',\n",
    "                                        kernel_initializer='lecun_normal',                                \n",
    "                                        recurrent_dropout=0.5))(decoder_l2,initial_state=encoder_states3)\n",
    "\n",
    "\n",
    "        decoder_outputs2 = TimeDistributed(Dense(2))(decoder_l3)\n",
    "\n",
    "        model = tf.keras.models.Model(encoder_inputs,decoder_outputs2)\n",
    "        model.compile(loss='mse',optimizer = opt,metrics = ['mse'])\n",
    "        \n",
    "        history = model.fit(train_X1, train_y1, \n",
    "                            epochs=1, \n",
    "                            verbose=0,\n",
    "                            batch_size=batch,\n",
    "                            shuffle=False,\n",
    "                            validation_split=0.1,\n",
    "                            callbacks=[reduce_lr])\n",
    "        \n",
    "        y_hat = model.predict(test_X1)\n",
    "\n",
    "        y_hat_inv = scaler1y.inverse_transform(y_hat).reshape(y_hat.shape[0],-1)\n",
    "        test_y1_inv = scaler1y.inverse_transform(test_y1).reshape(test_y1.shape[0],-1)\n",
    "\n",
    "        print(f'RMSE for the first day: {mean_squared_error(test_y1_inv[:,0],y_hat_inv[:,0],squared=False):.2f}')\n",
    "        print(f'MAE for the first day: {mean_absolute_error(test_y1_inv[:,0],y_hat_inv[:,0]):.2f}\\n')\n",
    "\n",
    "        print(f'RMSE for the second day: {mean_squared_error(test_y1_inv[:,1],y_hat_inv[:,1],squared=False):.2f}')\n",
    "        print(f'MAE for the second day: {mean_absolute_error(test_y1_inv[:,1],y_hat_inv[:,1]):.2f}\\n')\n",
    "\n",
    "        print(f'NSE: {he.nse(test_y1_inv,y_hat_inv)}\\n')\n",
    "        model_yaml = model.to_yaml()\n",
    "        with open(f\"bi_lstm118_s2s_lr{rate}_batch_{batch}.yaml\", \"w\") as yaml_file:\n",
    "            yaml_file.write(model_yaml)\n",
    "\n",
    "        model.save_weights(f\"bi_lstm118_s2s_lr{rate}_batch_{batch}.h5\")\n",
    "        print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
