{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.CommonFunctions import col_lagger,split_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hydroeval as he\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams[\"font.family\"] = 'Nanum Brush Script OTF'\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "CB91_Blue = '#2CBDFE'\n",
    "CB91_Green = '#47DBCD'\n",
    "CB91_Pink = '#F3A0F2'\n",
    "CB91_Purple = '#9D2EC5'\n",
    "CB91_Violet = '#661D98'\n",
    "CB91_Amber = '#F5B14C'\n",
    "\n",
    "color_list = [CB91_Blue, \n",
    "              CB91_Pink, \n",
    "              CB91_Green, \n",
    "              CB91_Amber,\n",
    "              CB91_Purple, \n",
    "              CB91_Violet]\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_lagger(df,x,num_lags):\n",
    "    df2 = copy.deepcopy(df)\n",
    "    for lag in range(1,num_lags+1):\n",
    "        df2[x+'_'+str(lag)] = df2[x].shift(lag)\n",
    "        \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..//Processed_data//weather_dam_wrn.csv',encoding='cp949',index_col=[0])\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['강우량(mm)+1']] = df[['강우량(mm)']].shift(-1)\n",
    "#df[['강우량(mm)+2']] = df[['강우량(mm)']].shift(-2)\n",
    "df = col_lagger(df,'강우량(mm)',7)\n",
    "df = col_lagger(df,'유입량(㎥/s)',7)\n",
    "df = col_lagger(df,'최저기온(°C)',7)\n",
    "df = col_lagger(df,'최고기온(°C)',7)\n",
    "df = col_lagger(df,'평균 풍속(m/s)',7)\n",
    "df = col_lagger(df,'호우경보',7)\n",
    "df = col_lagger(df,'평균 상대습도(%)',7)\n",
    "df = col_lagger(df,'합계 일사량(MJ/m2)',7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['최저기온(°C)', '최고기온(°C)', \n",
    "                      '평균 풍속(m/s)', '평균 상대습도(%)',\n",
    "                      '합계 일사량(MJ/m2)','호우경보'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[c for c in df.columns if c != '유입량(㎥/s)']].values\n",
    "y = df[['유입량(㎥/s)']].values\n",
    "\n",
    "data_stacked = np.hstack((X,y))\n",
    "\n",
    "del X\n",
    "del y\n",
    "X,y = split_sequences(data_stacked,1,2)\n",
    "train_length = int(df.shape[0]*0.8)\n",
    "\n",
    "train_X , train_y = X[:train_length, :] , y[:train_length, :]\n",
    "test_X , test_y = X[train_length:, :] , y[train_length:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-triple",
   "metadata": {},
   "source": [
    "# Scaling X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1x = StandardScaler()\n",
    "scaler1y = StandardScaler()\n",
    "\n",
    "#Scale X\n",
    "train_X_prescaled = train_X.reshape(train_X.shape[0],-1)\n",
    "train_X1 = scaler1x.fit_transform(train_X_prescaled).reshape(train_X_prescaled.shape[0],-1)#,train_X_prescaled.shape[-1])\n",
    "\n",
    "test_X_prescaled = test_X.reshape(test_X.shape[0],-1)\n",
    "test_X1 = scaler1x.transform(test_X_prescaled).reshape(test_X.shape[0],-1)#,test_X.shape[-1])\n",
    "\n",
    "#Scale y\n",
    "train_y_prescaled = train_y.reshape(train_y.shape[0],-1)\n",
    "train_y1 = scaler1y.fit_transform(train_y_prescaled).reshape(train_y.shape[0],-1)#,train_y.shape[-1])\n",
    "\n",
    "test_y_prescaled = test_y.reshape(test_y.shape[0],-1)\n",
    "test_y1 = scaler1y.transform(test_y_prescaled).reshape(test_y.shape[0],-1)#,test_y.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X1.shape)\n",
    "print(test_X1.shape)\n",
    "print(train_y1.shape)\n",
    "print(test_y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "gb = GradientBoostingRegressor()\n",
    "mlp = MLPRegressor()\n",
    "svr = SVR()\n",
    "\n",
    "\n",
    "\n",
    "#WRAPPER for RF, GB, MLP\n",
    "wrapper_rf = MultiOutputRegressor(rf)\n",
    "wrapper_gb = MultiOutputRegressor(gb)\n",
    "wrapper_mlp = MultiOutputRegressor(mlp)\n",
    "wrapper_svr = MultiOutputRegressor(svr)\n",
    "\n",
    "test_y1_inv = scaler1y.inverse_transform(test_y1)\n",
    "\n",
    "#grid search (SVR)\n",
    "svr_params = {\n",
    "    'estimator__kernel':['linear','poly','rbf'],\n",
    "    'estimator__degree':[2,3,4,5],\n",
    "    'estimator__gamma':['scale','auto']\n",
    "}\n",
    "\n",
    "grid_cv_svr = GridSearchCV(wrapper_svr,svr_params,n_jobs=-3)\n",
    "\n",
    "#grid search (Gradient Boosting)\n",
    "gb_params = {\n",
    "    'estimator__loss':['ls','lad','huber','quantile'],\n",
    "    'estimator__learning_rate':[0.1,0.01,0.001],\n",
    "    'estimator__n_estimators':[100,200,300],\n",
    "    'estimator__criterion':['friedman_mse','mse','mae']\n",
    "    \n",
    "}\n",
    "\n",
    "grid_cv_gb = GridSearchCV(wrapper_gb,gb_params,n_jobs=-3)\n",
    "\n",
    "#grid search (Random Forest)\n",
    "rf_params = {\n",
    "    'estimator__n_estimators':[100,200,500],\n",
    "    'estimator__criterion':['mse','mae'],\n",
    "    'estimator__max_features':['auto','sqrt','log2']\n",
    "}\n",
    "\n",
    "grid_cv_rf = GridSearchCV(wrapper_rf,rf_params,n_jobs=-3)\n",
    "\n",
    "#grid search (MLP)\n",
    "mlp_params = {\n",
    "    'estimator__hidden_layer_sizes':[(30,30,30),(50,50,50),(100,100,100)],\n",
    "    'estimator__activation':['identity','logistic','tanh','relu'],\n",
    "    'estimator__solver':['lbfgs','sgd','adam'],\n",
    "    'estimator__batch_size':[32,64,128],\n",
    "    'estimator__learning_rate':['constant','invscaling','adaptive'],\n",
    "    'estimator__shuffle':[False]\n",
    "}\n",
    "\n",
    "grid_cv_mlp = GridSearchCV(wrapper_mlp,mlp_params,n_jobs=-3)\n",
    "\n",
    "#MLP\n",
    "mlp2 = grid_cv_mlp.fit(train_X1,train_y1)\n",
    "mlp_pred = mlp2.predict(test_X1)\n",
    "mlp_pred_inv = scaler1y.inverse_transform(mlp_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('..... Multilayer Perceptron ..... ')\n",
    "print('*'*40)\n",
    "print(f'RMSE for first day: {mean_squared_error(test_y1_inv[:,0],mlp_pred_inv[:,0],squared=False)}')\n",
    "print(f'MAE for first day: {mean_absolute_error(test_y1_inv[:,0],mlp_pred_inv[:,0])}\\n')\n",
    "\n",
    "print(f'RMSE 2일: {mean_squared_error(test_y1_inv[:,1],mlp_pred_inv[:,1],squared=False)}')\n",
    "print(f'MAE 2일: {mean_absolute_error(test_y1_inv[:,1],mlp_pred_inv[:,1])}\\n')\n",
    "\n",
    "print(f'NSE : {he.nse(test_y1_inv,mlp_pred_inv)}\\n')\n",
    "print('*'*60)\n",
    "print()\n",
    "\n",
    "#Random Forest\n",
    "rf2 = grid_cv_rf.fit(train_X1,train_y1)\n",
    "rf_pred = rf2.predict(test_X1)\n",
    "rf_pred_inv = scaler1y.inverse_transform(rf_pred)\n",
    "print('.... Random Forest ....')\n",
    "print('*'*40)\n",
    "print(f'RMSE 1일: {mean_squared_error(test_y1_inv[:,0],rf_pred_inv[:,0],squared=False)}')\n",
    "print(f'MAE 1일: {mean_absolute_error(test_y1_inv[:,0],rf_pred_inv[:,0])}\\n')\n",
    "\n",
    "print(f'RMSE 2일: {mean_squared_error(test_y1_inv[:,1],rf_pred_inv[:,1],squared=False)}')\n",
    "print(f'MAE 2일: {mean_absolute_error(test_y1_inv[:,1],rf_pred_inv[:,1])}\\n')\n",
    "\n",
    "print(f'NSE : {he.nse(test_y1_inv,rf_pred_inv)}\\n')\n",
    "print('*'*60)\n",
    "print()\n",
    "\n",
    "#Gradient Boosting\n",
    "gb2 = grid_cv_gb.fit(train_X1,train_y1)\n",
    "gb_pred = gb2.predict(test_X1)\n",
    "gb_pred_inv = scaler1y.inverse_transform(gb_pred)\n",
    "print('...Gradient Boosting ....')\n",
    "print('*'*40)\n",
    "print(f'RMSE 1일: {mean_squared_error(test_y1_inv[:,0],gb_pred_inv[:,0],squared=False)}')\n",
    "print(f'MAE 1일: {mean_absolute_error(test_y1_inv[:,0],gb_pred_inv[:,0])}\\n')\n",
    "\n",
    "print(f'RMSE 2일: {mean_squared_error(test_y1_inv[:,1],gb_pred_inv[:,1],squared=False)}')\n",
    "print(f'MAE 2일: {mean_absolute_error(test_y1_inv[:,1],gb_pred_inv[:,1])}\\n')\n",
    "\n",
    "print(f'NSE : {he.nse(test_y1_inv,gb_pred_inv)}\\n')\n",
    "\n",
    "print('*'*60)\n",
    "print()\n",
    "\n",
    "#Support Vector Regressor\n",
    "svr2 = grid_cv_svr.fit(train_X1,train_y1)\n",
    "svr_pred = svr2.predict(test_X1)\n",
    "svr_pred_inv = scaler1y.inverse_transform(svr_pred)\n",
    "print(f'.... Support Vector Regressor ....')\n",
    "print('*'*40)\n",
    "print(f'RMSE 1일: {mean_squared_error(test_y1_inv[:,0],svr_pred_inv[:,0],squared=False)}')\n",
    "print(f'MAE 1일: {mean_absolute_error(test_y1_inv[:,0],svr_pred_inv[:,0])}')\n",
    "\n",
    "print(f'RMSE 2일: {mean_squared_error(test_y1_inv[:,1],svr_pred_inv[:,1],squared=False)}')\n",
    "print(f'MAE 2일: {mean_absolute_error(test_y1_inv[:,1],svr_pred_inv[:,1])}\\n')\n",
    "\n",
    "print(f'NSE : {he.nse(test_y1_inv,svr_pred_inv)}\\n')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameter for MLP')\n",
    "print(grid_cv_mlp.best_params_)\n",
    "print('*'*60)\n",
    "print()\n",
    "print('Best parameter for Gradient Boosting')\n",
    "print(grid_cv_gb.best_params_)\n",
    "print('*'*60)\n",
    "print()\n",
    "print('Best parameter for Random Forest')\n",
    "print(grid_cv_rf.best_params_)\n",
    "print('*'*60)\n",
    "print()\n",
    "print('Best parameter for Support Vector Regressor')\n",
    "print(grid_cv_svr.best_params_)\n",
    "print('*'*60)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-pocket",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
