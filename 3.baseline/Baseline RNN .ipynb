{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.CommonFunctions import col_lagger,split_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hydroeval as he\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "import os\n",
    "import warnings\n",
    "from keras.optimizers import *\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.callbacks import *\n",
    "from keras.models import model_from_yaml\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "CB91_Blue = '#2CBDFE'\n",
    "CB91_Green = '#47DBCD'\n",
    "CB91_Pink = '#F3A0F2'\n",
    "CB91_Purple = '#9D2EC5'\n",
    "CB91_Violet = '#661D98'\n",
    "CB91_Amber = '#F5B14C'\n",
    "\n",
    "color_list = [CB91_Blue, \n",
    "              CB91_Pink, \n",
    "              CB91_Green, \n",
    "              CB91_Amber,\n",
    "              CB91_Purple, \n",
    "              CB91_Violet]\n",
    "\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams[\"font.family\"] = 'Nanum Brush Script OTF'\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..//Processed_data//weather_dam_wrn.csv',encoding='cp949',index_col=[0])\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['강우량(mm)+1']] = df[['강우량(mm)']].shift(-1)\n",
    "#df[['강우량(mm)+2']] = df[['강우량(mm)']].shift(-2)\n",
    "df = col_lagger(df,'강우량(mm)',7)\n",
    "df = col_lagger(df,'유입량(㎥/s)',7)\n",
    "df = col_lagger(df,'최저기온(°C)',7)\n",
    "df = col_lagger(df,'최고기온(°C)',7)\n",
    "df = col_lagger(df,'평균 풍속(m/s)',7)\n",
    "df = col_lagger(df,'호우경보',7)\n",
    "df = col_lagger(df,'평균 상대습도(%)',7)\n",
    "df = col_lagger(df,'합계 일사량(MJ/m2)',7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['최저기온(°C)', '최고기온(°C)', \n",
    "                      '평균 풍속(m/s)', '평균 상대습도(%)',\n",
    "                      '합계 일사량(MJ/m2)','호우경보'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[c for c in df.columns if '유입량(㎥/s)'  != c]].values\n",
    "y = df[['유입량(㎥/s)']].values\n",
    "\n",
    "data_stacked = np.hstack((X,y))\n",
    "\n",
    "del X\n",
    "del y\n",
    "X,y = split_sequences(data_stacked,1,2)\n",
    "train_length = int(df.shape[0]*0.8)\n",
    "\n",
    "train_X , train_y = X[:train_length, :] , y[:train_length, :]\n",
    "test_X , test_y = X[train_length:, :] , y[train_length:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-dutch",
   "metadata": {},
   "source": [
    "# Scaling X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1x = StandardScaler()\n",
    "scaler1y = StandardScaler()\n",
    "\n",
    "#Scale X\n",
    "train_X_prescaled = train_X.reshape(train_X.shape[0],-1)\n",
    "train_X1 = scaler1x.fit_transform(train_X_prescaled).reshape(train_X_prescaled.shape[0],1,train_X_prescaled.shape[-1])\n",
    "\n",
    "test_X_prescaled = test_X.reshape(test_X.shape[0],-1)\n",
    "test_X1 = scaler1x.transform(test_X_prescaled).reshape(test_X.shape[0],1,test_X.shape[-1])\n",
    "\n",
    "#Scale y\n",
    "train_y_prescaled = train_y.reshape(train_y.shape[0],-1)\n",
    "train_y1 = scaler1y.fit_transform(train_y_prescaled).reshape(train_y.shape[0],1,train_y.shape[-1])\n",
    "\n",
    "test_y_prescaled = test_y.reshape(test_y.shape[0],-1)\n",
    "test_y1 = scaler1y.transform(test_y_prescaled).reshape(test_y.shape[0],1,test_y.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X1.shape)\n",
    "print(test_X1.shape)\n",
    "print(train_y1.shape)\n",
    "print(test_y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-psychiatry",
   "metadata": {},
   "source": [
    "# Grid Search for SimpleRNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [0.1,0.01,0.001]\n",
    "batches = [64,128,256]\n",
    "\n",
    "for batch in tqdm_notebook(batches, desc='Batch Size'):\n",
    "    for rate in tqdm_notebook(lr,leave=False, desc='Learning Rate'):\n",
    "        \n",
    "\n",
    "        model_input = Input(shape=(train_X1.shape[1], train_X1.shape[2]))\n",
    "        \n",
    "        RNN1 = SimpleRNN(59,\n",
    "                       return_sequences=True)(model_input)\n",
    "\n",
    "        \n",
    "        RNN2 = SimpleRNN(59,\n",
    "               return_sequences=True)(RNN1)\n",
    "\n",
    "        \n",
    "        RNN3= SimpleRNN(59, \n",
    "               return_sequences=True)(RNN2)\n",
    "            \n",
    "\n",
    "       \n",
    "        output = TimeDistributed(Dense(2,\n",
    "                  activation='linear'))(RNN3)\n",
    "\n",
    "        model = tf.keras.models.Model(model_input,output)\n",
    "        \n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=rate)\n",
    "        model.compile(loss='mse',optimizer=opt,metrics=['mse'])\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                      min_delta=0.1,\n",
    "                                      patience=5, min_lr=1e-5, vebose=1)\n",
    "        history = model.fit(train_X1, train_y1, \n",
    "                            epochs=3000, \n",
    "                            verbose=0,\n",
    "                            batch_size=batch,\n",
    "                            shuffle=False,\n",
    "                            validation_split=0.1,\n",
    "                            callbacks=[reduce_lr])\n",
    "\n",
    "        print(f'\\n\\nBatch Size: {batch}, Learning Rate:{rate}\\n\\n') \n",
    "        \n",
    "        y_hat = model.predict(test_X1)\n",
    "\n",
    "        y_hat_inv = scaler1y.inverse_transform(y_hat).reshape(y_hat.shape[0],-1)\n",
    "        test_y1_inv = scaler1y.inverse_transform(test_y1).reshape(test_y1.shape[0],-1)\n",
    "\n",
    "        print(f'RMSE for first day: {mean_squared_error(test_y1_inv[:,0],y_hat_inv[:,0],squared=False):.2f}')\n",
    "        print(f'MAE for first day: {mean_absolute_error(test_y1_inv[:,0],y_hat_inv[:,0]):.2f}\\n\\n')\n",
    "\n",
    "        print(f'RMSE for second day: {mean_squared_error(test_y1_inv[:,1],y_hat_inv[:,1],squared=False):.2f}')\n",
    "        print(f'MAE for second day: {mean_absolute_error(test_y1_inv[:,1],y_hat_inv[:,1]):.2f}\\n\\n')\n",
    "\n",
    "        print(f'NSE: {he.nse(test_y1_inv,y_hat_inv)}\\n')\n",
    "        model_yaml = model.to_yaml()\n",
    "        with open(f\"baselineRNN_lr{rate}_batch_{batch}.yaml\", \"w\") as yaml_file:\n",
    "            yaml_file.write(model_yaml)\n",
    "\n",
    "        model.save_weights(f\"baselineRNN_lr{rate}_batch_{batch}.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        print('*'*60)\n",
    "        del model\n",
    "        del y_hat\n",
    "        \n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-australian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
